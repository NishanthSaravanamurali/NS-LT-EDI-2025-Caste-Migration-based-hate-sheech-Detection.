{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fastext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download models if not already available\n",
    "fasttext.util.download_model('en', if_exists='ignore')\n",
    "fasttext.util.download_model('ta', if_exists='ignore')\n",
    "\n",
    "# Load models\n",
    "ft_model_en = fasttext.load_model('cc.en.300.bin')  # English\n",
    "ft_model_ta = fasttext.load_model('cc.ta.300.bin')  # Tamil\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    df['Class'] = df['Class'].str.lower()\n",
    "\n",
    "    def preprocess_text(text):\n",
    "        text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
    "        text = text.lower()  # Convert to lowercase\n",
    "        return text\n",
    "\n",
    "    df['Text_cleaned'] = df['Text'].apply(preprocess_text)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_fasttext_embeddings(texts, model):\n",
    "    \"\"\"\n",
    "    Generate sentence embeddings using FastText.\n",
    "\n",
    "    Args:\n",
    "        texts (list): List of input texts.\n",
    "        model: FastText model.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Embeddings for input texts.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    for text in tqdm(texts, desc=\"Generating FastText embeddings\"):\n",
    "        words = text.split()\n",
    "        word_vectors = [model.get_word_vector(word) for word in words if word in model.words]\n",
    "\n",
    "        if len(word_vectors) > 0:\n",
    "            sentence_embedding = np.mean(word_vectors, axis=0)  # Average word embeddings\n",
    "        else:\n",
    "            sentence_embedding = np.zeros(300)  # Handle empty text case\n",
    "\n",
    "        embeddings.append(sentence_embedding)\n",
    "\n",
    "    return torch.tensor(embeddings, dtype=torch.float32)\n",
    "\n",
    "\n",
    "def process_and_save_embeddings(csv_path, save_prefix, model, use_third_col=False):\n",
    "    \"\"\"\n",
    "    Process a dataset by generating FastText embeddings and saving them.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to the CSV file.\n",
    "        save_prefix (str): Prefix for saving the embeddings and labels.\n",
    "        model: FastText model.\n",
    "        use_third_col (bool): Whether to use the 'Translated' column (for _eng datasets).\n",
    "    \"\"\"\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Select text column based on dataset type\n",
    "    \n",
    "    texts = df[\"Translated\"].tolist()\n",
    "    df.drop(columns=[\"text\"], inplace=True)  # Drop the original column\n",
    "    \n",
    "\n",
    "    # Convert labels to numerical format\n",
    "    #df['Class'] = df['Class'].str.lower()\n",
    "    #df[\"Class\"] = df[\"Class\"].map({'abusive': 1, 'non-abusive': 0})\n",
    "    labels = df[\"label\"].to_numpy()\n",
    "\n",
    "    # Generate embeddings\n",
    "    print(f\"Generating embeddings for {csv_path}...\")\n",
    "    embeddings = get_fasttext_embeddings(texts, model)\n",
    "\n",
    "    # Save embeddings and labels\n",
    "    np.save(f\"{save_prefix}_fasttext_embeddings.npy\", embeddings.numpy())\n",
    "    np.savetxt(f\"{save_prefix}_labels.txt\", labels, fmt='%d')\n",
    "\n",
    "    print(f\"Saved embeddings and labels for {csv_path} as {save_prefix}_*.npy / .txt\")\n",
    "\n",
    "def process_and_save_embeddings2(csv_path, save_prefix, model, use_third_col=False):\n",
    "    \"\"\"\n",
    "    Process a dataset by generating FastText embeddings and saving them.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to the CSV file.\n",
    "        save_prefix (str): Prefix for saving the embeddings and labels.\n",
    "        model: FastText model.\n",
    "        use_third_col (bool): Whether to use the 'Translated' column (for _eng datasets).\n",
    "    \"\"\"\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Select text column based on dataset type\n",
    "    \n",
    "    texts = df[\"Translated\"].tolist()\n",
    "    df.drop(columns=[\"text\"], inplace=True)  # Drop the original column\n",
    "\n",
    "    # Convert labels to numerical format\n",
    "    #df['Class'] = df['Class'].str.lower()\n",
    "    #df[\"Class\"] = df[\"Class\"].map({'abusive': 1, 'non-abusive': 0})\n",
    "    #labels = df[\"label\"].to_numpy()\n",
    "\n",
    "    # Generate embeddings\n",
    "    print(f\"Generating embeddings for {csv_path}...\")\n",
    "    embeddings = get_fasttext_embeddings(texts, model)\n",
    "\n",
    "    # Save embeddings and labels\n",
    "    np.save(f\"{save_prefix}_fasttext_embeddings.npy\", embeddings.numpy())\n",
    "\n",
    "    print(f\"Saved embeddings for {csv_path} as _*.npy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for E:\\PROJECT\\Speech\\3\\translated_sentences_deep_translator_train_english.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating FastText embeddings: 100%|██████████| 5512/5512 [05:27<00:00, 16.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings and labels for E:\\PROJECT\\Speech\\3\\translated_sentences_deep_translator_train_english.csv as train_tamil_eng_*.npy / .txt\n",
      "Generating embeddings for E:\\PROJECT\\Speech\\3\\translated_sentences_deep_translator_dev_english.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating FastText embeddings: 100%|██████████| 787/787 [01:23<00:00,  9.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings and labels for E:\\PROJECT\\Speech\\3\\translated_sentences_deep_translator_dev_english.csv as dev_tamil_eng_*.npy / .txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define language models\n",
    "language_models = {\n",
    "    \"English\": ft_model_en,\n",
    "    \"Tamil\": ft_model_ta,\n",
    "}\n",
    "\n",
    "# Datasets with language as string keys to lookup the correct FastText model\n",
    "datasets = [\n",
    "  #  (\"train_tamil\", r\"E:\\PROJECT\\Speech\\3\\translated_sentences_deep_translator_train_tamil.csv\", False, \"Tamil\"),\n",
    "#    (\"dev_tamil\", r\"E:\\PROJECT\\Speech\\3\\translated_sentences_deep_translator_dev_tamil.csv\", False, \"Tamil\"),\n",
    "#    (\"test_tamil\", r\"E:\\PROJECT\\Speech\\3\\translated_sentences_deep_translator_test_tamil.csv\", False, \"Tamil\"),\n",
    "\n",
    "    (\"train_tamil_eng\", r\"E:\\PROJECT\\Speech\\3\\translated_sentences_deep_translator_train_english.csv\", True, \"English\"),\n",
    "    (\"dev_tamil_eng\", r\"E:\\PROJECT\\Speech\\3\\translated_sentences_deep_translator_dev_english.csv\", True, \"English\"),\n",
    "#    (\"test_tamil_eng\", r\"E:\\PROJECT\\Speech\\3\\translated_sentences_deep_translator_dev_tamil.csv\", True, \"English\"),\n",
    "]\n",
    "\n",
    "# Process each dataset using the correct model\n",
    "for name, path, is_eng, lang_key in datasets:\n",
    "    ft_model = language_models[lang_key]\n",
    "    process_and_save_embeddings(path, name, ft_model, use_third_col=is_eng)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for E:\\PROJECT\\Speech\\3\\translated_sentences_deep_translator_test_tamil.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating FastText embeddings: 100%|██████████| 1576/1576 [06:30<00:00,  4.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings for E:\\PROJECT\\Speech\\3\\translated_sentences_deep_translator_test_tamil.csv as _*.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\nisha\\AppData\\Local\\Temp\\ipykernel_22412\\94703843.py:38: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  return torch.tensor(embeddings, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# Define language models\n",
    "language_models = {\n",
    "    \"English\": ft_model_en,\n",
    "    \"Tamil\": ft_model_ta,\n",
    "}\n",
    "\n",
    "# Datasets with language as string keys to lookup the correct FastText model\n",
    "datasets = [\n",
    "#    (\"train_tamil\", r\"E:\\PROJECT\\Speech\\3\\translated_sentences_deep_translator_train_tamil.csv\", False, \"Tamil\"),\n",
    "#    (\"dev_tamil\", r\"E:\\PROJECT\\Speech\\3\\translated_sentences_deep_translator_dev_tamil.csv\", False, \"Tamil\"),\n",
    " #   (\"test_tamil\", r\"E:\\PROJECT\\Speech\\3\\translated_sentences_deep_translator_test_tamil.csv\", False, \"Tamil\"),\n",
    "\n",
    "#    (\"train_tamil_eng\", r\"E:\\PROJECT\\Speech\\3\\translated_sentences_deep_translator_dev_tamil.csv\", True, \"English\"),\n",
    "#    (\"dev_tamil_eng\", r\"E:\\PROJECT\\Speech\\3\\translated_sentences_deep_translator_dev_tamil.csv\", True, \"English\"),\n",
    "    (\"test_tamil_eng\", r\"E:\\PROJECT\\Speech\\3\\translated_sentences_deep_translator_test_tamil.csv\", True, \"English\"),\n",
    "]\n",
    "\n",
    "# Process each dataset using the correct model\n",
    "for name, path, is_eng, lang_key in datasets:\n",
    "    ft_model = language_models[lang_key]\n",
    "    process_and_save_embeddings2(path, name, ft_model, use_third_col=is_eng)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
